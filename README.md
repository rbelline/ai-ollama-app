This is a project showing how to embed LLM locally on edge using Ollama models and fine tuning copabilities.

The use case is reading pdf, create a vector db and e return the results of the query as a generative response
